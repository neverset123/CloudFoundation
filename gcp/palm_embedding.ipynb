{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "om0L7SpIkezD"
   },
   "source": [
    "# Using PaLM to Cluster Products Based on Descriptions\n",
    "\n",
    "In this lab, you're going to use a list of product descriptions to create a model that will cluster products together. To train the clustering model, you need to find the meaning of the text in the descriptions. You'll take advantage of the language understanding of the PaLM model to generate the embedded meanings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcKYLauBqxKT",
    "tags": []
   },
   "source": [
    "## Setup\n",
    "To access the PaLM Embeddings, you need to set up the client to access Vertex AI. Start by installing one of the latest versions of the Python client library.\n",
    "\n",
    "All of the imports will be done up front to keep the notebook tidy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Yj86jOepszm"
   },
   "source": [
    "\n",
    "### Installs\n",
    "Make sure we have a new enough version of the Vertex AI Python client library to use the `textembeddings-gecko` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rahbv6G3A5u9",
    "outputId": "d3f51d3f-1230-430b-f539-e7d3c6090d7f"
   },
   "outputs": [],
   "source": [
    "!pip3 install \"google-cloud-aiplatform>=1.27\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the install, we need to restart the kernel to use the new library version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoiMMtRdlj-p"
   },
   "source": [
    "### Imports\n",
    "Import all of the modules needed for the notebook.\n",
    "\n",
    "Most modules in the Python client library belong to the package `google.cloud`, but Vertex AI is an exception.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TK8osmKGB90I"
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import google.auth\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from typing import Generator, List, Optional, Tuple\n",
    "import functools\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import pickle\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0kIBlZamszU"
   },
   "source": [
    "### User Authentication\n",
    "If you are using Colab, uncomment this cell and run it to authenticate as your user for Google Cloud.\n",
    "\n",
    "Shortcut: Select the cell contents and press `Ctrl/Cmd + /`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0pHFht6Dyz9"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nPKRJx2n2V0"
   },
   "source": [
    "Run the cell below to retrieve the credentials for intializing access to Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4IJy1q43moMm"
   },
   "outputs": [],
   "source": [
    "credentials, _ = google.auth.default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpV7k9eJnB3g"
   },
   "source": [
    "### Client setup\n",
    "\n",
    "Configure your project ID in `MY_PROJECT`.\n",
    "\n",
    "If you want to use a specific Vertex AI location, set it as `VERTEX_LOCATION`. We'll default to us-central1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZvSlFSMpbWs"
   },
   "outputs": [],
   "source": [
    "MY_PROJECT='YOUR-PROJECT-ID-HERE'\n",
    "VERTEX_LOCATION='us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20jsc9ZlBLUb"
   },
   "outputs": [],
   "source": [
    "vertexai.init(\n",
    "    project=MY_PROJECT,\n",
    "    location=VERTEX_LOCATION,\n",
    "    #credentials set above\n",
    "    credentials=credentials\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVceXVB5rx9I"
   },
   "source": [
    "## Data\n",
    "The product description data we are using was gathered for the paper\n",
    "\n",
    "**Justifying recommendations using distantly-labeled reviews and fined-grained aspects**\\\n",
    "Jianmo Ni, Jiacheng Li, Julian McAuley\\\n",
    "_Empirical Methods in Natural Language Processing (EMNLP)_, 2019\\\n",
    "[pdf](http://cseweb.ucsd.edu/~jmcauley/pdfs/emnlp19a.pdf)\n",
    "\n",
    "This [dataset](https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/index.html) includes many additional fields that you're not going to use, including reviews from users. To keep this lab more focused, the data has already been parsed and cleaned. We've taken a sample of 2,000 products each across 5 categories, for a total of 10,000 products. The clustering model you'll train will be used to attempt to rediscover those categories!\n",
    "\n",
    "The cleaned data only includes the name of the item and the description. This is formatted into a CSV file, which is hosted on Cloud Storage.\n",
    "\n",
    "The five original categories are as follows:\n",
    "* all beauty\n",
    "* appliances\n",
    "* musical instruments\n",
    "* pantry\n",
    "* software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CL2wsxJjR9pA"
   },
   "outputs": [],
   "source": [
    "URL = 'https://storage.googleapis.com/cloud-training/specialized-training/model_garden/products.csv.gz'\n",
    "products_df = pd.read_csv(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoRsQQ7hFu_b"
   },
   "source": [
    "You can preview the data in the following cell. Verify that there are indeed 10,000 rows of data consisting of two columns: name and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Y3pvqtTWVi-j",
    "outputId": "f26aa7e4-7f84-42ee-eecf-8befdb558b43"
   },
   "outputs": [],
   "source": [
    "products_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "citgca62VrG3",
    "outputId": "a08fd0db-7a91-490f-eb71-1c15d540383c"
   },
   "outputs": [],
   "source": [
    "len(products_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fij3WI3p0dJ"
   },
   "source": [
    "## Embeddings Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhdpjoJfGKzb"
   },
   "source": [
    "With the product information loaded, you can now get the embeddings for the description. Start by loading the model. Check the model card in Model Garden for how to get started, or view the API documentation.\n",
    "\n",
    "[PaLM Text Embeddings Model Card](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/textembedding-gecko)\n",
    "\n",
    "[TexEmbeddingModel API](https://cloud.google.com/python/docs/reference/aiplatform/latest/vertexai.language_models.TextEmbeddingModel)\n",
    "\n",
    "[TextEmbeddingModel code](https://github.com/googleapis/python-aiplatform/blob/main/vertexai/language_models/_language_models.py#L510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ftbeq7vBRhC"
   },
   "outputs": [],
   "source": [
    "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYENqbBkHW1E"
   },
   "source": [
    "Confirm that we are using the model from Model Garden that we expect, which is the TextEmbeddingModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RGJv1qTbEqyR",
    "outputId": "8e8c863f-b43d-4c8b-b6ea-31c27fc00810"
   },
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3hKDOiEGjeh"
   },
   "source": [
    "Let's try calling the model with a couple example sentences, just to see the basic process. We'll print the first 15 dimensions of each vector to see what the result looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bgt4XOgzDOmC",
    "outputId": "5dc9ca47-f7b9-4608-bcd9-c779d343c198"
   },
   "outputs": [],
   "source": [
    "embeddings = model.get_embeddings([\"Dinner in New York City\", \"Dinner in Paris\"])\n",
    "for embedding in embeddings:\n",
    "    vector = embedding.values\n",
    "    print(vector[:15])\n",
    "    print('\\n-----\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ppUMX7zYG7ml",
    "outputId": "e7878f1b-141e-415f-8217-1d69d0c3e898"
   },
   "outputs": [],
   "source": [
    "print(f'Response type: {type(embeddings)}')\n",
    "print(f'Each response type: {type(embeddings[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hh6TJ-iVGz0Y"
   },
   "source": [
    "The Python client puts the responses from the model into a list in order of the requested sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zpm1RlMLHLqe",
    "outputId": "19e08bbd-91b3-432c-f178-4201ec4df49a"
   },
   "outputs": [],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgjOu2ieb7wz"
   },
   "source": [
    "The TextEmbeddings are a set of floating point values representing the 768 dimensions used by PaLM for understanding the meaning of text. Later, we'll convert these lists of floats to numpy ndarrays for easier use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_aH-3daHg2p",
    "outputId": "a071a97e-7226-417a-f6d4-480a5f0e03a1"
   },
   "outputs": [],
   "source": [
    "print(f'Values: {len(embeddings[0].values)}')\n",
    "print(f'Value type: {type(embeddings[0].values[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wL_f1-nH5Si"
   },
   "source": [
    "According to the [documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings#get_text_embeddings_for_a_snippet_of_text), the API has a limit of 5 input texts (product descriptions in our case) per API call. With a batch size of 5 and 10,000 product descriptions to embed, we'll need to make 2,000 calls to the API. It's time to turn the basic example into a function for more utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71syMMrEXLu-"
   },
   "source": [
    "#### Function to call the model\n",
    "This function does what our basic example does, which is call the `get_embeddings` method to convert our text. It adds very basic error handling, which should be expanded for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "980yTXSPXCT2"
   },
   "outputs": [],
   "source": [
    "# Define an embedding method that uses the model\n",
    "def encode_texts_to_embeddings(sentences: List[str]) -> List[Optional[List[float]]]:\n",
    "    try:\n",
    "        embeddings = model.get_embeddings(sentences)\n",
    "        return [embedding.values for embedding in embeddings]\n",
    "    except Exception:\n",
    "        return [None for _ in range(len(sentences))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ya9y9rXPKK5P"
   },
   "source": [
    "#### Define two more helper functions for converting text to embeddings\n",
    "\n",
    "- generate_batches:  This method splits `sentences` into batches of 5 before sending to the embedding API.\n",
    "- encode_text_to_embedding_batched: This method calls `generate_batches` to handle batching and then calls the embedding API via `encode_texts_to_embeddings`. It also handles rate-limiting using `time.sleep`. For production use cases, you would want a more sophisticated rate-limiting mechanism that takes retries into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfO_IE3cKOWn"
   },
   "outputs": [],
   "source": [
    "# Generator function to yield batches of descriptions\n",
    "def generate_batches(\n",
    "    descriptions: List[str], batch_size: int\n",
    ") -> Generator[List[str], None, None]:\n",
    "    for i in range(0, len(descriptions), batch_size):\n",
    "        yield descriptions[i : i + batch_size]\n",
    "\n",
    "\n",
    "def encode_text_to_embedding_batched(\n",
    "    descriptions: List[str], api_calls_per_minute: int = 20, batch_size: int = 5\n",
    ") -> Tuple[List[bool], np.ndarray]:\n",
    "\n",
    "    embeddings_list: List[List[float]] = []\n",
    "\n",
    "    # Prepare the batches using a generator\n",
    "    batches = generate_batches(descriptions, batch_size)\n",
    "\n",
    "    seconds_per_job = 60 / api_calls_per_minute\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for batch in tqdm(\n",
    "            batches, total=math.ceil(len(descriptions) / batch_size), position=0\n",
    "        ):\n",
    "            futures.append(\n",
    "                executor.submit(functools.partial(encode_texts_to_embeddings), batch)\n",
    "            )\n",
    "            time.sleep(seconds_per_job)\n",
    "\n",
    "        for future in futures:\n",
    "            embeddings_list.extend(future.result())\n",
    "\n",
    "    is_successful = [\n",
    "        embedding is not None for sentence, embedding in zip(descriptions, embeddings_list)\n",
    "    ]\n",
    "    embeddings_list_successful = np.squeeze(\n",
    "        np.stack([embedding for embedding in embeddings_list if embedding is not None])\n",
    "    )\n",
    "    return is_successful, embeddings_list_successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpbR-pIxd0jw"
   },
   "source": [
    "#### Generate Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkV9gat23jmc"
   },
   "source": [
    "To generate the embeddings, we will call the batch helper function. However, the Qwiklabs environment has a low rate quota, so this process will take a long time to complete. Luckily, an important principle of the embeddings is that they won't substantially change no matter when you do the conversion! We can compute the embeddings once, store them, then load them whenever we need them again.\n",
    "\n",
    "The below cell is the code for generating the embeddings. It is commented out since you won't be using it, but it does show you the process for your own use later! Make sure to change the `api_calls_per_minute` parameter based on [your own quota rate limit!](https://console.cloud.google.com/iam-admin/quotas?referrer=search&pageState=(%22allQuotasTable%22:(%22f%22:%22%255B%257B_22k_22_3A_22_22_2C_22t_22_3A10_2C_22v_22_3A_22_5C_22base_model_3Atextembedding-gecko_5C_22_22_2C_22s_22_3Atrue%257D%255D%22)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "639ed7d1fc2449419e9adfc1efbb9fa6",
      "48db3d8ca8d14e0ebef708aa4a270c22",
      "6c6e34ee0d0a4fb69c168b9b34738a73",
      "90cdce26d7834e9b81747b2d224e69e3",
      "f5c46ed3021a4db9a14ea5d10dcb0615",
      "366467fb9a6c409e9a2d0311d93b68fd",
      "54169e2dbec64f20b380de669b591d34",
      "0bb79938f69b41fe8b6976cd5bb7332d",
      "be1a4562bfde46ae8eea31b45e3f5f7a",
      "05e05253caba4266931b2c4d35b16944",
      "5797768f09414ed49d673fe0bb21573b"
     ]
    },
    "id": "XK_qEE1bWlxS",
    "outputId": "0582a89a-09e3-4fde-b6b7-30e3d9c59758"
   },
   "outputs": [],
   "source": [
    "# descriptions = products_df['description'].values.tolist()\n",
    "# response = encode_text_to_embedding_batched(descriptions, api_calls_per_minute=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "263Foe2v43bx"
   },
   "source": [
    "The full response is hosted on Cloud Storage so we can import it and pick up right where the previous cell would have left us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2Kl3jbI7Zuq"
   },
   "outputs": [],
   "source": [
    "URL = 'https://storage.googleapis.com/cloud-training/specialized-training/model_garden/embeddings-response.pkl'\n",
    "response = pickle.load(urlopen(URL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMwOxSgiXsYz"
   },
   "source": [
    "The `response` created by the `encode_text_to_embeddings_batched` function is a Tuple of (List[bool], ndarray)\n",
    "\n",
    "The first tuple item represents whether the API call was successful or not. There should be an equal number of responses to the size of the original product description list (10,000). These should all be True to indicate we have embeddings for every description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JG3pK5F9Xmm_",
    "outputId": "929f87b3-ee7b-460d-b024-837fe337a3ea"
   },
   "outputs": [],
   "source": [
    "print('Item 0')\n",
    "print(f'\\ttype: {type(response[0])}')\n",
    "print(f'\\tvalues: {len(response[0])}')\n",
    "# Test to make sure all responses are True.\n",
    "print(f'All embeddings requests completed successfully: {all(response[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pSBtah4Bh4R"
   },
   "source": [
    "The second tuple item is a numpy ndarray of the 768-dimension embedding vectors provided by the model for each product description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0C60kW2N-2ft",
    "outputId": "11c9ce86-030b-475e-91ed-cfcd6c9e8379"
   },
   "outputs": [],
   "source": [
    "print('Item 1')\n",
    "print(f'\\ttype: {type(response[1])}')\n",
    "print(f'\\tshape: {response[1].shape}')\n",
    "print(f'\\tvector data type: {response[1].dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Uxkd_JGEXgt"
   },
   "source": [
    "Even though the giant string of floating point numbers doesn't mean anything to us as humans, it's still interesting to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SYZ5l-w6DXxz",
    "outputId": "4e45cb9f-9406-4854-8656-aefb564ad3db"
   },
   "outputs": [],
   "source": [
    "print(f'Original text: \\n{products_df[\"description\"][0]}\\nEmbedding vector:')\n",
    "print(response[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbhzVEaBDYR2"
   },
   "source": [
    "## Clustering Model\n",
    "With the embeddings computed, you can use those to create categories representing similar products. Notice that the dataset you loaded is not labeled for which category each product belongs to. However, those product categories do exist the source data.\n",
    "\n",
    "You'll use a K-Means model to find similarities in the embedding vectors. Using scikit-learn, the [KMeans model](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) directly accepts an ndarray, which our embedding vectors are formatted as thanks to the helper functions!\n",
    "\n",
    "As mentioned, the sample data came from 5 product categories, so we're going to have the model learn 5 clusters. With the small dataset we're using, training the clustering model will take almost no time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCKth5J5aKr7"
   },
   "outputs": [],
   "source": [
    "embeddings = response[1]\n",
    "kmeans = KMeans(n_clusters=5, n_init=\"auto\").fit(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlP8i8DCf1q7"
   },
   "source": [
    "Fitting the model also provides us with the predictions for the values used to train the model in the `labels_` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LSJtpHokeqtS",
    "outputId": "ce0ced9e-dd26-4d77-9948-7c2f24deacf1"
   },
   "outputs": [],
   "source": [
    "len(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UinH9O6lgisw"
   },
   "source": [
    "We can sanity check that by calling the `predict` function directly, which is also how you'd cluster future products now that we've trained the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0qyf686gRtb"
   },
   "outputs": [],
   "source": [
    "predictions = kmeans.predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aA-o8YrNfm9A",
    "outputId": "1f99d627-2713-4f66-d5b9-8ee223b3d27f"
   },
   "outputs": [],
   "source": [
    "kmeans.labels_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RN02b-jUgWNP",
    "outputId": "d3f9ac26-fcbf-4a5b-aa86-c58949d8c89a"
   },
   "outputs": [],
   "source": [
    "predictions[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDo_U_JVoBK6"
   },
   "source": [
    "#### Evaluate the clusters\n",
    "Let's attach the predicted clusters to the original data and see some example products from each cluster to see how well the model performed.\n",
    "\n",
    "As a reminder, the data has 2,000 products from each of 5 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35Je8UUExUDl"
   },
   "outputs": [],
   "source": [
    "products_df['category'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_6N0PT3hxguQ",
    "outputId": "516f6380-9fb2-47f7-d077-659689bbfae3"
   },
   "outputs": [],
   "source": [
    "products_df.groupby('category').count()['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjuXrMVFF2Oy"
   },
   "source": [
    "We can see that the model did not entirely accurately recreate the categories as there are not exactly 2,000 in each cluster. However, the performance isn't terrible for having spent about one second training.\n",
    "\n",
    "What you should also know about the input data is that it was sorted by category. Each block of 2,000 rows is one product category. Let's see how well the model predicts each of the different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96gptzVUHguq",
    "outputId": "53a35b8a-ea06-4eef-fc55-d5d552aa9241"
   },
   "outputs": [],
   "source": [
    "# Create named slices for easier reference\n",
    "beauty = slice(2000)\n",
    "appliance = slice(2000,4000)\n",
    "instrument = slice(4000,6000)\n",
    "pantry = slice(6000,8000)\n",
    "software = slice(8000,10000)\n",
    "\n",
    "beauty_cnts = Counter(predictions[beauty])\n",
    "appliance_cnts = Counter(predictions[appliance])\n",
    "instrument_cnts = Counter(predictions[instrument])\n",
    "pantry_cnts = Counter(predictions[pantry])\n",
    "software_cnts = Counter(predictions[software])\n",
    "\n",
    "product_pred_cnts = [beauty_cnts, appliance_cnts, instrument_cnts, pantry_cnts, software_cnts]\n",
    "for product_cnt in product_pred_cnts:\n",
    "    print(product_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuiAA6jEDyPa"
   },
   "source": [
    "With the set of predictions and the counts per category, we can now label the clusters based on the highest number of predictions. With many clustering tasks, we won't have a source of ground truth like in this example, so figuring out names for your clusters will be up to you. You'll do additional analysis of the features of the objects that are predicted to be in the same cluster by your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TRW36uWOOmCY",
    "outputId": "69577fa1-eb46-452d-834e-9a3e0cbb8499"
   },
   "outputs": [],
   "source": [
    "def most_likely_cluster(counted_dict):\n",
    "    return max(counted_dict, key=counted_dict.get)\n",
    "\n",
    "product_category_list = ['all_beauty', 'appliance', 'instrument', 'pantry', 'software']\n",
    "\n",
    "product_clusters = []\n",
    "for product in product_pred_cnts:\n",
    "    product_clusters.append(most_likely_cluster(product))\n",
    "\n",
    "keys = [0, 1, 2, 3, 4] # simple list of index values as the products are inserted\n",
    "row_names = dict(zip(keys, product_category_list))\n",
    "cluster_map = dict(zip(product_clusters, product_category_list))\n",
    "print(row_names) # ascending alpha order\n",
    "print(cluster_map) # predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkrWATrAMA4o"
   },
   "source": [
    "Add all of that information to a new DataFrame for easier visualization. The prediction counts get added in as raw information, then we'll update row and column names to match the product categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfsvsDKBH_gJ"
   },
   "outputs": [],
   "source": [
    "correctness_df = pd.DataFrame.from_dict(product_pred_cnts)\n",
    "correctness_df.rename(index=row_names, inplace=True)\n",
    "correctness_df.rename(columns=cluster_map, inplace=True)\n",
    "correctness_df.sort_index(axis=0, inplace=True)\n",
    "correctness_df.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "bCsHdgwiJC-C",
    "outputId": "c708a195-b4cb-48be-f95b-f328034a17b6"
   },
   "outputs": [],
   "source": [
    "correctness_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tqy4cDdPRg_6"
   },
   "source": [
    "From this confusion matrix, we can see which categories have the most recognizable patterns of descriptions. Down the diagonal is the count of correct predictions.\n",
    "\n",
    "With this data, the model predicted software and instruments very well. Very few of those predictions were wrong. However, it also frequently confused appliances and beauty products for instruments. You can see which descriptions were confusing by selecting those products from the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument_category = product_clusters[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "fr0XMuedSqc4",
    "outputId": "ac65554c-0dd2-47a3-b682-77a98f554eb1"
   },
   "outputs": [],
   "source": [
    "products_df.iloc[beauty].query(f'category == {instrument_category}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZET0y5ULckg-"
   },
   "source": [
    "We can compare those descriptions with the descriptions from instruments that were correctly predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "RzuvpR7LchRe",
    "outputId": "b9c2c1c5-8d5e-426b-a4d2-8f2db1fb5975"
   },
   "outputs": [],
   "source": [
    "products_df.iloc[instrument].query(f'category == {instrument_category}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5Eq7F46dFwC"
   },
   "source": [
    "They don't look that similar to me, but they apparently look similar to the model at the embedding vector level!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdQb0V_ZhelM"
   },
   "source": [
    "### Save the model\n",
    "Having the model available in the notebook is all well and good, but right now, it only lives in memory and is only accessible from within the notebook. That's not very useful. We can do better. First, let's save the model locally on the machine to prevent us from losing it during a reboot. We can use Pickle to export the model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0N8P4zNnFW9"
   },
   "outputs": [],
   "source": [
    "model_local_file = 'model.pkl'\n",
    "with open(model_local_file, 'wb') as model_file:\n",
    "    pickle.dump(kmeans, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBk-yuVOn1hI"
   },
   "source": [
    "At least now the model is persisted to local storage, but it's still not very available. A much better place to store it is Cloud Storage!\n",
    "\n",
    "We'll default to using a Cloud Storage bucket name based on our project ID. You can set this to any Cloud Storage bucket you have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2PUNXE1peaI"
   },
   "outputs": [],
   "source": [
    "GCS_BUCKET=f\"{MY_PROJECT}-productcluster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PiCCwjzmqxJJ"
   },
   "outputs": [],
   "source": [
    "gcs_client = storage.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKz4sgb_prqv"
   },
   "source": [
    "The following cell will create the bucket. If you set the variable to an existing bucket, skip the following cell or it will throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgLA6OZ5pi6a"
   },
   "outputs": [],
   "source": [
    "gcs_client.create_bucket(GCS_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7z0HBkgLqZS2"
   },
   "source": [
    "Now we can upload the model to Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SwIauffopyt"
   },
   "outputs": [],
   "source": [
    "bucket = gcs_client.get_bucket(GCS_BUCKET)\n",
    "gcs_model_path = f'PaLM_embeddings_product_cluster'\n",
    "blob = bucket.blob(f'{gcs_model_path}/{model_local_file}')\n",
    "blob.upload_from_filename(model_local_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a53AFJ1g2cDI"
   },
   "source": [
    "To reload your model for future use in a notebook, simply retrive the pickled model file from Cloud Storage, then use pickle to load it again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rEnfAYvyqJsK",
    "outputId": "d324d702-0b62-4b4b-8bee-11406aead227"
   },
   "outputs": [],
   "source": [
    "with open(model_local_file, 'rb') as f:\n",
    "    reloaded_kmeans = pickle.load(f)\n",
    "\n",
    "reloaded_kmeans.predict(embeddings[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIANpwVHrA9W"
   },
   "source": [
    "Since we still have the original in memory, we can easily show the predictions from the reloaded model are the same as the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zdnmUELnq6IJ",
    "outputId": "8e83858a-095a-413b-9584-6701aeffb280"
   },
   "outputs": [],
   "source": [
    "kmeans.predict(embeddings[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibUj5szdroPg"
   },
   "source": [
    "### Model Registry\n",
    "Being in Cloud Storage means you're going to have 11-9s of durability so you know the model won't be lost, and now you can download it in other notebooks to use.\n",
    "\n",
    "However, we can still do better!\n",
    "\n",
    "You can't use the model directly from Cloud Storage to make predictions. You'd have to spin up a notebook or other server, retrieve the model, then load the model back into memory to use it again. That's a lot of work, and it doesn't let you use this model for any kind of real-time predictions.\n",
    "\n",
    "Instead, wouldn't it be great if we could directly send embeddings to our model and not have to worry about the computing behind it? By adding the model to Model Registry, we can unlock the full power of Vertex AI to run our model at scale!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KdI6lEs7Txy"
   },
   "source": [
    "According to the [documentation](https://cloud.google.com/vertex-ai/docs/training/exporting-model-artifacts#scikit-learn), Vertex AI expects the model to be named `model.pkl` when importing to Model Registry, which is why we named it that above. To register the model, you pass in the directory (really, key prefix) for where Model Registry can find the pickled model in Cloud Storage. You also specify a serving container image, which your model will be injected into to run on Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2FxO0LB5rzg3",
    "outputId": "66e24641-f838-4de8-d051-f6ccd1650081"
   },
   "outputs": [],
   "source": [
    "model_display_name = 'cluster_products_using_palm'\n",
    "artifact_directory_uri = f'gs://{GCS_BUCKET}/{gcs_model_path}'\n",
    "serving_container_image = 'us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-2:latest'\n",
    "\n",
    "vai_model = aiplatform.Model.upload(\n",
    "            display_name=model_display_name,\n",
    "            artifact_uri=artifact_directory_uri,\n",
    "            serving_container_image_uri=serving_container_image)\n",
    "\n",
    "vai_model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qs2wPuz1p6WX"
   },
   "source": [
    "Now you can easily send batch predictions to your model, or publish it to an Endpoint for real-time applications!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9Q4plQOhX87"
   },
   "source": [
    "### Use the model\n",
    "Finally, with your model no longer in danger of being lost, and being substantially more usable in Model Registry, let's turn back to our local copy of the model to predict a few more products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pI6xVO35iW52"
   },
   "source": [
    "Here are a few more cleaned-up product descriptions from the original dataset that were not included in the sample, one from each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnKkQhXJiMv-"
   },
   "outputs": [],
   "source": [
    "new_products = ['Kinetronics StaticWisk Brush-7/8',\n",
    "                'Range Kleen GE/Hotpoint Large Porcelain Drip Bowl 8\"',\n",
    "                'Game of Thrones (Theme from the HBO series) - EASY PIANO Sheet Music Single',\n",
    "                'CADBURY Chocolate Candy Bar, English Toffee, 5.4 Ounce',\n",
    "                'Star Trek: The Game Show']\n",
    "new_descriptions = ['Fine quality anti-static lens brush. Made from a special blend of soft, natural hair and a conductive fiber. To use the brush, simply sweep the lens. The brush has a resistivity of 10-1 and will dissipate any static charge and release the dust.',\n",
    "                    'Are your burner reflector bowls beyond rescue? Reasonably priced and easy to clean, replacement bowls will have your stove looking spiffy in a jiffy!',\n",
    "                    'EASY Piano version of the popular theme song from the HBO series.',\n",
    "                    'Enjoy the rich taste of premium milk chocolate with the satisfying crunch of English toffee. Make any moment more delicious with an Cadbury milk chocolate with English toffee pouch.',\n",
    "                    'Star Trek: The Game Show'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyr9SXF6pdjM"
   },
   "source": [
    "First, generate the text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdaUOSTVpUeK"
   },
   "outputs": [],
   "source": [
    "new_embeddings = model.get_embeddings(new_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJcr5lbxpugU"
   },
   "source": [
    "The returned embeddings are in a TextEmbedding class. Convert that to an ndarray of floats to pass to the clustering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-1s10iMptBB"
   },
   "outputs": [],
   "source": [
    "new_embeddings_nd = np.squeeze(np.stack([embedding.values for embedding in new_embeddings if embedding is not None]))\n",
    "new_predictions = kmeans.predict(new_embeddings_nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BHA2NbvIs5JA",
    "outputId": "5a7ac7e5-c15f-4f12-ff5f-71c23339d95b"
   },
   "outputs": [],
   "source": [
    "new_predictions_clusters = [cluster_map[x] for x in new_predictions]\n",
    "new_predictions_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mljgw9OPuZwl"
   },
   "source": [
    "How did your model do?\n",
    "\n",
    "When I ran this, the model got 4 of 5 correct. Not bad! It confused the musical instrument category for software, which was the most likely category for it to be wrong about based on my confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-c_LhprOhPje"
   },
   "source": [
    "## Congratulations!\n",
    "\n",
    "In this lab, you've converted a set of product descriptions into a vector representation using the PaLM Embeddings model. With that contextual information, you trained a clustering model to predict the category of the product based on the description. You then persisted the model to Cloud Storage, and most powerfully, added it to Model Registry. It's now ready to use in your Vertex AI pipelines!\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m116"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05e05253caba4266931b2c4d35b16944": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0bb79938f69b41fe8b6976cd5bb7332d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "366467fb9a6c409e9a2d0311d93b68fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48db3d8ca8d14e0ebef708aa4a270c22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_366467fb9a6c409e9a2d0311d93b68fd",
      "placeholder": "​",
      "style": "IPY_MODEL_54169e2dbec64f20b380de669b591d34",
      "value": "100%"
     }
    },
    "54169e2dbec64f20b380de669b591d34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5797768f09414ed49d673fe0bb21573b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "639ed7d1fc2449419e9adfc1efbb9fa6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_48db3d8ca8d14e0ebef708aa4a270c22",
       "IPY_MODEL_6c6e34ee0d0a4fb69c168b9b34738a73",
       "IPY_MODEL_90cdce26d7834e9b81747b2d224e69e3"
      ],
      "layout": "IPY_MODEL_f5c46ed3021a4db9a14ea5d10dcb0615"
     }
    },
    "6c6e34ee0d0a4fb69c168b9b34738a73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0bb79938f69b41fe8b6976cd5bb7332d",
      "max": 2000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_be1a4562bfde46ae8eea31b45e3f5f7a",
      "value": 2000
     }
    },
    "90cdce26d7834e9b81747b2d224e69e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05e05253caba4266931b2c4d35b16944",
      "placeholder": "​",
      "style": "IPY_MODEL_5797768f09414ed49d673fe0bb21573b",
      "value": " 2000/2000 [03:22&lt;00:00,  9.89it/s]"
     }
    },
    "be1a4562bfde46ae8eea31b45e3f5f7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f5c46ed3021a4db9a14ea5d10dcb0615": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
